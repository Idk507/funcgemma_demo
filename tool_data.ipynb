{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ea63265",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhanu\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\dhanu\\miniconda3\\envs\\idk\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n",
      "W1231 21:20:58.258000 31636 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c6f098c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1+cu126\n",
      "0.24.1+cu126\n",
      "2.9.1+cu126\n",
      "12.6\n"
     ]
    }
   ],
   "source": [
    "import torch, torchvision, torchaudio\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n",
    "print(torchaudio.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "711ee074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58c61f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 4096 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da0713f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"google/functiongemma-270m-it\", device_map=\"auto\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/functiongemma-270m-it\", dtype=\"auto\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bfa91f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/functiongemma-270m-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c182ae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import peft\n",
    "from peft import LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f3b3a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=128,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=128*2,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    use_rslora=False,\n",
    "\n",
    ")\n",
    "\n",
    "model = peft.get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88a31f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Example 1: Basic turns ===\n",
      "<bos><start_of_turn>developer\n",
      "You are a helpful assistant.<end_of_turn>\n",
      "<start_of_turn>user\n",
      "Hello, who are you?<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages_1 = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello, who are you?\"},\n",
    "]\n",
    "\n",
    "rendered_1 = tokenizer.apply_chat_template(\n",
    "    messages_1,\n",
    "    tools = [], # no tools\n",
    "    add_generation_prompt = False,\n",
    "    tokenize = False,\n",
    ")\n",
    "\n",
    "print(\"=== Example 1: Basic turns ===\")\n",
    "print(rendered_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c39edd7",
   "metadata": {},
   "source": [
    " <start_function_declaration>declaration:get_weather{...}<end_function_declaration> encodes the full function spec (name, description, parameters) so the model knows what tools it can call and how to format arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90064ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Example 2: Tool declarations ===\n",
      "<bos><start_of_turn>developer\n",
      "You are a weather assistant.<start_function_declaration>declaration:get_weather{description:<escape>Get the current weather for a given city.<escape>,parameters:{properties:{city:{description:<escape>City name, e.g. 'Tokyo'.<escape>,type:<escape>STRING<escape>}},required:[<escape>city<escape>],type:<escape>OBJECT<escape>}}<end_function_declaration><end_of_turn>\n",
      "<start_of_turn>user\n",
      "What is the weather in Tokyo?<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tools_2 = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Get the current weather for a given city.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"city\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"City name, e.g. 'Tokyo'.\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"city\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "messages_2 = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a weather assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the weather in Tokyo?\"},\n",
    "]\n",
    "\n",
    "rendered_2 = tokenizer.apply_chat_template(\n",
    "    messages_2,\n",
    "    tools = tools_2,\n",
    "    add_generation_prompt = False,\n",
    "    tokenize = False,\n",
    ")\n",
    "\n",
    "print(\"=== Example 2: Tool declarations ===\")\n",
    "print(rendered_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36f3cabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Example 3: User â†’ Model â†’ Tool â†’ Model ===\n",
      "<bos><start_of_turn>developer\n",
      "You are a weather assistant.<start_function_declaration>declaration:get_weather{description:<escape>Get the current weather for a given city.<escape>,parameters:{properties:{city:{description:<escape>City name, e.g. 'Tokyo'.<escape>,type:<escape>STRING<escape>}},required:[<escape>city<escape>],type:<escape>OBJECT<escape>}}<end_function_declaration><end_of_turn>\n",
      "<start_of_turn>user\n",
      "What is the weather in Tokyo?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "<start_function_call>call:get_weather{city:<escape>Tokyo<escape>}<end_function_call><start_function_response>response:get_weather{value:<escape>{\"city\": \"Tokyo\", \"temp_c\": 25, \"condition\": \"sunny\"}<escape>}<end_function_response>It is currently 25Â°C and sunny in Tokyo.<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages_3 = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a weather assistant.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is the weather in Tokyo?\",\n",
    "    },\n",
    "    # Assistant issues a tool call\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"\",\n",
    "        \"tool_calls\": [\n",
    "            {\n",
    "                \"id\": \"call_1\",\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"get_weather\",\n",
    "                    \"arguments\": {\"city\": \"Tokyo\"},\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    # Tool (infrastructure) responds\n",
    "    {\n",
    "        \"role\": \"tool\",\n",
    "        \"name\": \"get_weather\",\n",
    "        \"tool_call_id\": \"call_1\",\n",
    "        \"content\": '{\"city\": \"Tokyo\", \"temp_c\": 25, \"condition\": \"sunny\"}',\n",
    "    },\n",
    "    # Assistant gives final natural-language answer\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"It is currently 25Â°C and sunny in Tokyo.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "rendered_3 = tokenizer.apply_chat_template(\n",
    "    messages_3,\n",
    "    tools = tools_2,\n",
    "    add_generation_prompt = False,\n",
    "    tokenize = False,\n",
    ")\n",
    "\n",
    "print(\"=== Example 3: User â†’ Model â†’ Tool â†’ Model ===\")\n",
    "print(rendered_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4800065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Thinking + Tools ===\n",
      "<bos><start_of_turn>developer\n",
      "You are a shopping assistant. Use tools when you need detailed Amazon product data such as price and specifications.<start_function_declaration>declaration:get_amazon_product_details{description:<escape>Retrieves comprehensive product information from Amazon, including title, price, description, specifications, and availability.<escape>,parameters:{properties:{asin:{description:<escape>The Amazon ASIN of the product.<escape>,type:<escape>STRING<escape>}},required:[<escape>asin<escape>],type:<escape>OBJECT<escape>}}<end_function_declaration><end_of_turn>\n",
      "<start_of_turn>user\n",
      "Is the espresso machine with ASIN B0XYZ12345 any good for home use?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "<think>User is asking for an opinion, but I need factual product details first such as price, features, and reviews. I should call the Amazon product details tool with the provided ASIN.</think><start_function_call>call:get_amazon_product_details{asin:<escape>B0XYZ12345<escape>}<end_function_call><start_function_response>response:get_amazon_product_details{value:<escape>{\"title\": \"Home Pro Espresso 3000\", \"price\": 199.99, \"pressure_bar\": 15, \"features\": [\"steam wand\", \"single and double shot baskets\"], \"pros\": [\"good crema\", \"compact\"], \"cons\": [\"a bit noisy\"]}<escape>}<end_function_response><think>Tool response shows a mid-range price and standard 15 bar pressure. Features and pros/cons indicate itâ€™s fine for home espresso but not a high-end machine for enthusiasts.</think>\n",
      "Based on the product details, the Home Pro Espresso 3000 (ASIN B0XYZ12345) is a solid option for home use. It offers 15-bar pressure, a steam wand, and both single and double shot baskets, which are enough for everyday lattes and cappuccinos. Itâ€™s compact and produces good crema, but it can be a bit noisy. If you want a convenient, reasonably priced home machine, it should work well; if youâ€™re very picky about espresso or plan to upgrade grinders and accessories, you might eventually want something more advanced.<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tools_4 = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_amazon_product_details\",\n",
    "            \"description\": (\n",
    "                \"Retrieves comprehensive product information from Amazon, \"\n",
    "                \"including title, price, description, specifications, and availability.\"\n",
    "            ),\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"asin\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The Amazon ASIN of the product.\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"asin\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "messages_4 = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a shopping assistant. Use tools when you need detailed \"\n",
    "            \"Amazon product data such as price and specifications.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Is the espresso machine with ASIN B0XYZ12345 any good for home use?\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": (\n",
    "            \"<think>\"\n",
    "            \"User is asking for an opinion, but I need factual product details first \"\n",
    "            \"such as price, features, and reviews. I should call the Amazon product \"\n",
    "            \"details tool with the provided ASIN.\"\n",
    "            \"</think>\"\n",
    "        ),\n",
    "        \"tool_calls\": [\n",
    "            {\n",
    "                \"id\": \"call_1\",\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"get_amazon_product_details\",\n",
    "                    \"arguments\": {\n",
    "                        \"asin\": \"B0XYZ12345\"\n",
    "                    },\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"tool\",\n",
    "        \"name\": \"get_amazon_product_details\",\n",
    "        \"tool_call_id\": \"call_1\",\n",
    "        \"content\": (\n",
    "            '{\"title\": \"Home Pro Espresso 3000\", '\n",
    "            '\"price\": 199.99, '\n",
    "            '\"pressure_bar\": 15, '\n",
    "            '\"features\": [\"steam wand\", \"single and double shot baskets\"], '\n",
    "            '\"pros\": [\"good crema\", \"compact\"], '\n",
    "            '\"cons\": [\"a bit noisy\"]}'\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": (\n",
    "            \"<think>\"\n",
    "            \"Tool response shows a mid-range price and standard 15 bar pressure. \"\n",
    "            \"Features and pros/cons indicate itâ€™s fine for home espresso but not \"\n",
    "            \"a high-end machine for enthusiasts.\"\n",
    "            \"</think>\\n\"\n",
    "            \"Based on the product details, the Home Pro Espresso 3000 (ASIN B0XYZ12345) \"\n",
    "            \"is a solid option for home use. It offers 15-bar pressure, a steam wand, \"\n",
    "            \"and both single and double shot baskets, which are enough for everyday \"\n",
    "            \"lattes and cappuccinos. Itâ€™s compact and produces good crema, but it can \"\n",
    "            \"be a bit noisy. If you want a convenient, reasonably priced home machine, \"\n",
    "            \"it should work well; if youâ€™re very picky about espresso or plan to upgrade \"\n",
    "            \"grinders and accessories, you might eventually want something more advanced.\"\n",
    "        ),\n",
    "    },\n",
    "]\n",
    "\n",
    "rendered_prompt = tokenizer.apply_chat_template(\n",
    "    messages_4,\n",
    "    tools = tools_4,\n",
    "    add_generation_prompt = False,  # True if you want to open a fresh model turn for generation\n",
    "    tokenize = False,\n",
    ")\n",
    "\n",
    "print(\"=== Thinking + Tools ===\")\n",
    "print(rendered_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1e2c9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"LLM360/TxT360-3efforts\", name = \"agent\", split = \"medium\", streaming = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98c33a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_list(list(dataset.take(10000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6a69fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68676206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"role\": \"system\", \"tools\": [{\"name\": \"qrcodepro\", \"description\": \"Generates a professional QR code with customizable properties and settings.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"text\": {\"description\": \"The text to encode into the QR code.\", \"type\": \"string\", \"default\": \"https://www.digicatech.com\"}, \"validate\": {\"description\": \"Whether to validate the input text. Defaults to None.\", \"type\": \"string\", \"default\": true}, \"setlabel\": {\"description\": \"Whether to set a label on the QR code. Defaults to None.\", \"type\": \"string\", \"default\": false}, \"forecolor\": {\"description\": \"The foreground color of the QR code in hexadecimal format without the # prefix. Defaults to \\'000000\\'.\", \"type\": \"string\", \"default\": \"000000\"}, \"type\": {\"description\": \"The output file type for the QR code (\\'png\\', \\'svg\\', or \\'eps\\'). Defaults to \\'svg\\'.\", \"type\": \"string\", \"default\": \"svg\"}, \"labeltext\": {\"description\": \"The text to use as a label in the QR code. Defaults to None.\", \"type\": \"string\", \"default\": \"\"}, \"size\": {\"description\": \"The size of the output image in pixels, with a maximum of 4000. Defaults to 150.\", \"type\": \"string\", \"default\": \"150\"}, \"labelalign\": {\"description\": \"The alignment of the label (\\'left\\', \\'right\\', or \\'center\\'). Defaults to \\'center\\'.\", \"type\": \"string\", \"default\": \"center\"}, \"backcolor\": {\"description\": \"The background color of the QR code in hexadecimal format without the # prefix. Defaults to \\'FFFFFF\\'.\", \"type\": \"string\", \"default\": \"FFFFFF\"}, \"level\": {\"description\": \"The error correction level of the QR code (\\'L\\', \\'M\\', \\'Q\\', \\'H\\'). Defaults to \\'M\\'.\", \"type\": \"string\", \"default\": \"M\"}}}}, {\"name\": \"get_products_in_category\", \"description\": \"Fetches products in a specified category from a remote catalog using the RapidAPI service.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"category\": {\"description\": \"The category from which to fetch products.\", \"type\": \"string\", \"default\": \"\"}, \"limit\": {\"description\": \"The number of products to retrieve.\", \"type\": \"integer\", \"default\": \"\"}, \"skip\": {\"description\": \"The number of products to skip before starting to retrieve.\", \"type\": \"integer\", \"default\": \"\"}}}}, {\"name\": \"uuidv3_generator\", \"description\": \"Generates a UUIDv3 based on the provided namespace and name using the Toolbench RapidAPI service.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"namespace\": {\"description\": \"The namespace to use for generating the UUIDv3. Allowed values are \\'dns\\', \\'url\\', \\'oid\\', \\'x500\\', and \\'nil\\'.\", \"type\": \"string\", \"default\": \"nil\"}, \"name\": {\"description\": \"The name to use for generating the UUIDv3.\", \"type\": \"string\", \"default\": \"test\"}}}}, {\"name\": \"get_product\", \"description\": \"Fetches product details from the RapidAPI endpoint using the provided product ID and API key.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"is_id\": {\"description\": \"The product ID to fetch details for.\", \"type\": \"integer\", \"default\": \"\"}}}}]}, {\"role\": \"user\", \"content\": \"Generate a QR code for \\'My App\\' with a size of 300 and a label saying \\'Download Now\\', and also retrieve details for product 11121.\"}, {\"role\": \"assistant\", \"think_fast\": \"We need to do two tasks: generate QR code with given parameters and retrieve product details for product ID 11121.\\\\n\\\\nWe have two functions: qrcodepro and get_product. So need to call both.\\\\n\\\\nFirst, generate QR code: text likely is \\'My App\\'? Actually QR code text to encode is \\'My App\\'. size 300. set label: setlabel true? There\\'s setlabel param (maybe boolean). labeltext param \\'Download Now\\'. labelalign maybe default center. forecolor default black, etc.\\\\n\\\\nWe\\'ll call qrcodepro with text=\\'My App\\', size=\\'300\\', setlabel=\\'true\\' (maybe string? but default type seems string). labeltext=\\'Download Now\\'.\\\\n\\\\nSecond, call get_product with is_id=11121.\\\\n\\\\nWe need to output function calls then provide final answer summarizing.\\\\n\\\\n\", \"tool_calls\": [{\"name\": \"qrcodepro\", \"arguments\": \"{\\\\n  \\\\\"text\\\\\": \\\\\"My App\\\\\",\\\\n  \\\\\"size\\\\\": \\\\\"300\\\\\",\\\\n  \\\\\"setlabel\\\\\": \\\\\"true\\\\\",\\\\n  \\\\\"labeltext\\\\\": \\\\\"Download Now\\\\\"\\\\n}\"}]}]'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d19f516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "521d8f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "THINK_TAG_OPEN = \"<think>\"\n",
    "THINK_TAG_CLOSE = \"</think>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2b38dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Helper Function: prepare_messages_and_tools\n",
    "\n",
    "import json\n",
    "\n",
    "THINK_TAG_OPEN = \"<think>\"\n",
    "THINK_TAG_CLOSE = \"</think>\"\n",
    "\n",
    "def prepare_messages_and_tools(example):\n",
    "    raw = json.loads(example[\"messages\"])\n",
    "    msgs = [dict(m) for m in raw]\n",
    "\n",
    "    # 1) Extract tools (same as before)\n",
    "    tools_raw = []\n",
    "    if msgs and isinstance(msgs[0], dict):\n",
    "        tlist = msgs[0].get(\"tools\")\n",
    "        if isinstance(tlist, list) and tlist:\n",
    "            tools_raw = tlist\n",
    "            msgs[0].pop(\"tools\", None)\n",
    "\n",
    "    # 2) Merge assistant[\"think\"] into [\"content\"]\n",
    "    THINK_KEYS = [\"think\", \"think_fast\", \"think_faster\"]\n",
    "\n",
    "    # TRACKER: Check if we successfully added thoughts\n",
    "    has_valid_thought = False\n",
    "\n",
    "    for m in msgs:\n",
    "        if m.get(\"role\") == \"assistant\":\n",
    "            # Find the first available thinking key\n",
    "            found_key = next((k for k in THINK_KEYS if m.get(k)), None)\n",
    "\n",
    "            if found_key:\n",
    "                think_text = m[found_key]\n",
    "                content = m.get(\"content\")\n",
    "                think_block = f\"{THINK_TAG_OPEN}{think_text}{THINK_TAG_CLOSE}\"\n",
    "\n",
    "                if isinstance(content, str) and content:\n",
    "                    m[\"content\"] = think_block + \"\\n\" + content\n",
    "                else:\n",
    "                    m[\"content\"] = think_block\n",
    "\n",
    "                has_valid_thought = True\n",
    "\n",
    "                # Clean up keys\n",
    "                for k in THINK_KEYS:\n",
    "                    m.pop(k, None)\n",
    "            else:\n",
    "                # If an assistant message HAS NO THOUGHT,\n",
    "                # this example is \"poison\" for your goal.\n",
    "                # We mark it as invalid to filter it out later.\n",
    "                return None, None\n",
    "\n",
    "    # If the conversation had no assistant turns at all (rare, but possible), skip it\n",
    "    if not has_valid_thought:\n",
    "        return None, None\n",
    "    # 3) Normalize tool_calls to HF-style {type:'function', function:{name, arguments}}\n",
    "    for m in msgs:\n",
    "        if \"tool_calls\" not in m or not m[\"tool_calls\"]:\n",
    "            continue\n",
    "\n",
    "        new_tool_calls = []\n",
    "        for tc in m[\"tool_calls\"]:\n",
    "            if not isinstance(tc, dict):\n",
    "                continue\n",
    "\n",
    "            # Already has function dict?\n",
    "            if \"function\" in tc and isinstance(tc[\"function\"], dict):\n",
    "                new_tool_calls.append(tc)\n",
    "                continue\n",
    "\n",
    "            fn_name = tc.get(\"name\", \"\")\n",
    "            args = tc.get(\"arguments\", {})\n",
    "\n",
    "            # Try to parse JSON string arguments\n",
    "            if isinstance(args, str):\n",
    "                try:\n",
    "                    args = json.loads(args)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            new_tool_calls.append(\n",
    "                {\n",
    "                    \"id\": tc.get(\"id\") or tc.get(\"tool_call_id\"),\n",
    "                    \"type\": tc.get(\"type\", \"function\"),\n",
    "                    \"function\": {\n",
    "                        \"name\": fn_name,\n",
    "                        \"arguments\": args,\n",
    "                    },\n",
    "                }\n",
    "            )\n",
    "\n",
    "        m[\"tool_calls\"] = new_tool_calls\n",
    "\n",
    "    # 3b) Build map from tool_call_id -> function name for later tool responses\n",
    "    id_to_name = {}\n",
    "    for m in msgs:\n",
    "        for tc in m.get(\"tool_calls\", []) or []:\n",
    "            if not isinstance(tc, dict):\n",
    "                continue\n",
    "            fn = tc.get(\"function\") or {}\n",
    "            name = fn.get(\"name\") or tc.get(\"name\")\n",
    "            tc_id = tc.get(\"id\") or tc.get(\"tool_call_id\")\n",
    "            if tc_id and name:\n",
    "                id_to_name[tc_id] = name\n",
    "\n",
    "    # 3c) Ensure tool response messages have a 'name'\n",
    "    for m in msgs:\n",
    "        if m.get(\"role\") == \"tool\":\n",
    "            if not m.get(\"name\"):\n",
    "                # Try to infer from tool_call_id using previous map\n",
    "                tc_id = m.get(\"tool_call_id\")\n",
    "                inferred = id_to_name.get(tc_id) if tc_id else None\n",
    "                m[\"name\"] = inferred or \"unknown_tool\"\n",
    "\n",
    "    # 4) Normalize tool schemas to HF-style {type:'function', function:{...}}\n",
    "    adapted_tools = []\n",
    "    for t in tools_raw:\n",
    "        if not isinstance(t, dict):\n",
    "            continue\n",
    "\n",
    "        if \"function\" in t and isinstance(t[\"function\"], dict):\n",
    "            adapted_tools.append(t)\n",
    "            continue\n",
    "\n",
    "        name = t.get(\"name\", \"\")\n",
    "        description = t.get(\"description\", \"\")\n",
    "        parameters = t.get(\"parameters\") or {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {},\n",
    "        }\n",
    "\n",
    "        adapted_tools.append(\n",
    "            {\n",
    "                \"type\": t.get(\"type\", \"function\"),\n",
    "                \"function\": {\n",
    "                    \"name\": name,\n",
    "                    \"description\": description,\n",
    "                    \"parameters\": parameters,\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Delete empty system message\n",
    "    first_message = msgs[0]\n",
    "    if first_message[\"role\"] == \"system\" and \"content\" not in first_message:\n",
    "        msgs.pop(0)\n",
    "\n",
    "    return msgs, adapted_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5a5cde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:03<00:00, 3330.53 examples/s]\n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:00<00:00, 98089.66 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size after filtering: 4699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def format_example(example):\n",
    "    messages, tools = prepare_messages_and_tools(example)\n",
    "\n",
    "    # FILTER: If the preparation returned None, this example was bad.\n",
    "    if messages is None or len(messages) == 0:\n",
    "        return {\"text\": None}\n",
    "\n",
    "    chat_str = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tools = tools,\n",
    "        add_generation_prompt = False,\n",
    "        tokenize = False,\n",
    "    ).removeprefix(\"<bos>\")\n",
    "\n",
    "    return {\n",
    "        \"text\": chat_str,\n",
    "    }\n",
    "\n",
    "# Apply the map\n",
    "train_dataset = dataset.map(format_example)\n",
    "\n",
    "# Filter out the None values\n",
    "train_dataset = train_dataset.filter(lambda x: x[\"text\"] is not None)\n",
    "\n",
    "print(f\"Dataset size after filtering: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25de9a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\dhanu\\miniconda3\\envs\\idk\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer,SFTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8887d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding EOS to train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4699/4699 [00:00<00:00, 7701.13 examples/s]\n",
      "Tokenizing train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4699/4699 [00:08<00:00, 563.13 examples/s] \n",
      "Truncating train dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4699/4699 [00:00<00:00, 115960.74 examples/s]\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    # tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = None, # Can set up evaluation!\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 4,\n",
    "        gradient_accumulation_steps = 2, # Use GA to mimic batch size!\n",
    "        warmup_steps = 10,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 500,\n",
    "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.001,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use TrackIO/WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c942701d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6ddf8e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhanu\\AppData\\Local\\Temp\\ipykernel_31636\\697443393.py:1: UserWarning: WARNING: Unsloth should be imported before [trl, transformers, peft] to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth.chat_templates import train_on_responses_only\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4699/4699 [00:38<00:00, 122.75 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<start_of_turn>user\\n\",\n",
    "    response_part = \"<start_of_turn>model\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "39f1d80a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<bos><start_of_turn>developer\\nYou are K2, a helpful assistant created by Mohamed bin Zayed University of Artificial Intelligence (MBZUAI) Institute of Foundation Models (IFM).<start_function_declaration>declaration:final_velocity{description:<escape>Calculates the final velocity of an object given its initial velocity, acceleration, and time.<escape>,parameters:{properties:{acceleration:{description:<escape>The acceleration of the object.<escape>,type:<escape>NUMBER<escape>},initial_velocity:{description:<escape>The initial velocity of the object.<escape>,type:<escape>NUMBER<escape>},time:{description:<escape>The time elapsed.<escape>,type:<escape>NUMBER<escape>}},type:<escape>OBJECT<escape>}}<end_function_declaration><start_function_declaration>declaration:permutation_count{description:<escape>Calculates the number of permutations of k elements from a set of n elements.<escape>,parameters:{properties:{k:{description:<escape>The number of elements to choose for the permutation.<escape>,type:<escape>INTEGER<escape>},n:{description:<escape>The total number of elements in the set.<escape>,type:<escape>INTEGER<escape>}},type:<escape>OBJECT<escape>}}<end_function_declaration><end_of_turn>\\n<start_of_turn>user\\nWhat is the final velocity of a bike that starts at 3 m/s and accelerates at 0.5 m/sÂ² for 10 minutes? Also, calculate the number of ways to arrange 7 items out of 15.<end_of_turn>\\n<start_of_turn>model\\n<think>We need to compute final velocity using v = v0 + a*t. Initial velocity 3 m/s, acceleration 0.5 m/s^2, time 10 minutes => convert to seconds: 10*60 = 600 s. So final velocity = 3 + 0.5*600 = 3 + 300 = 303 m/s.\\n\\nNumber of permutations of 7 items out of 15: P(15,7) = 15! / (15-7)! = 15! / 8! Compute: 15*14*13*12*11*10*9 = let's compute step.\\n\\nCompute: 15*14 =210\\n210*13 = 2730\\n2730*12 = 32760\\n32760*11 = 360360\\n360360*10 = 3,603,600\\n3,603,600*9 = 32,432,400\\n\\nThus permutations = 32,432,400 ways.\\n\\nWe can also verify using tool for permutation count. I'll call the function to be precise. Also we can call final_velocity function.\\n\\nLet's call final_velocity with initial_velocity 3, acceleration 0.5, time 600.</think><start_function_call>call:final_velocity{acceleration:0.5,initial_velocity:3,time:600}<end_function_call><start_function_response><eos>\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(trainer.train_dataset[-1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "30a5e03c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------<think>User wants DNA sequence with ID HQ664368, format FASTA, upstream 250 bases. Use function get_dna_sequence with sequence_id \"HQ664368\", file_format \"fasta\", upstream_bases \"250\".</think><start_function_call>call:get_dna_sequence{file_format:<escape>fasta<escape>,sequence_id:<escape>HQ664368<escape>,upstream_bases:<escape>250<escape>}<end_function_call><start_function_response><eos>']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[100][\"labels\"]]).replace(tokenizer.pad_token, \"-\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc2982f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "torch._dynamo.reset()\n",
    "\n",
    "# Disable torch.compile completely for now\n",
    "torch.set_float32_matmul_precision('high')\n",
    "torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8967644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TORCHDYNAMO_DISABLE'] = '1'\n",
    "os.environ['TRITON_CACHE_DIR'] = 'C:/Users/dhanu/triton_cache'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "853c41aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 3050 Laptop GPU. Max memory = 4.0 GB.\n",
      "0.668 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e00e5210",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.32 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 8.82 GiB is allocated by PyTorch, and 27.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m trainer_stats = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dhanu\\miniconda3\\envs\\idk\\Lib\\site-packages\\transformers\\trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dhanu\\miniconda3\\envs\\idk\\Lib\\site-packages\\transformers\\trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dhanu\\miniconda3\\envs\\idk\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:1189\u001b[39m, in \u001b[36mSFTTrainer.training_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1187\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m   1188\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.maybe_activation_offload_context:\n\u001b[32m-> \u001b[39m\u001b[32m1189\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dhanu\\miniconda3\\envs\\idk\\Lib\\site-packages\\transformers\\trainer.py:4071\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   4068\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   4069\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4071\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4073\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dhanu\\miniconda3\\envs\\idk\\Lib\\site-packages\\accelerate\\accelerator.py:2734\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2732\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2733\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2734\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dhanu\\miniconda3\\envs\\idk\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dhanu\\miniconda3\\envs\\idk\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dhanu\\miniconda3\\envs\\idk\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 2.32 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 8.82 GiB is allocated by PyTorch, and 27.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b518ac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8450883",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages, tools = prepare_messages_and_tools(train_dataset[0])\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages[:1],\n",
    "    tools = tools,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    ").removeprefix('<bos>')\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 1024,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = False),\n",
    "    top_p = 0.95, top_k = 64, temperature = 1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae30121",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"functiongemma\")  # Local saving\n",
    "tokenizer.save_pretrained(\"functiongemma\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
